수정사항 및 추가사항 존재하는 함수들 기록

should_migration
ㄴmigration 판단(어째서인지 실제로 안쓰임)

should_migration_high
ㄴ강제 migration 판단(이것만 쓰임)

init_lines_slc
ㄴ lm 말고도 slm까지 초기화

remove_lines_slc
ㄴ lm 말고도 slm의 lines까지 remove

init_write_flow_control_slc
ㄴ slc와 tlc의 write_credits은 다름(왜냐하면 라인 당 페이지 수가 다르기 때문)

get_next_free_line_slc
ㄴ lm에서 받아오던 free_line을 slm으로부터 받도록 함

get_lines_slc
ㄴ lines는 slc/tlc로 구분되었고 그래서 라인 인덱싱 방식에 변화가 생김

select_victim_line
ㄴ 과제1인데 이거 그대로 쓰면 될 거 같음
ㄴ victim_line은 
   ㄴ slm에서 고르면 그건 migration 대상이고
   ㄴ lm에서 고르면 그건 gc 대상이고
   ㄴ lm은 greedy를 디폴트로 해야될텐데 이거 어케 구분하지 흠....
       ㄴ 수정이 필요해보임
           ㄴ 걍 select_victim_line은 gc가 디폴트(greedy)로 쓰도록 놔두고 select_victim_line_slc을 새로 만듬

select_victim_line_slc
ㄴ 과제 1 복붙인데 호출은 migration에서만 일어남
    ㄴ 매크로만 수정해서 과제 1이랑 독립적으로 되도록..(매크로에 넘버링함)
    ㄴ +FIFO도 한다던데 큐 추가해야 할 듯 이건 나중에

prepare_write_pointer_slc
ㄴ write_pointer가 free_line을 얻어낼 때, io_type에 따라 SLC line / TLC line을 구분해서 받아내야 함

advance_write_pointer_slc
ㄴ write_pointer가 line 내에서 advance될 때의 최대 line 길이를 (SLC/TLC)에 따라 구분하기 위함

spp->pgs_per_line = spp->blks_per_line * spp->pgs_per_blk;
	spp->blks_per_line = spp->tt_luns;
		spp->tt_luns = spp->luns_per_ch * spp->nchs;
			spp->luns_per_ch = LUNS_PER_NAND_CH;
				#define LUNS_PER_NAND_CH (4)
			spp->nchs = NAND_CHANNELS;
				#define NAND_CHANNELS (4)
	spp->pgs_per_blk = spp->pgs_per_oneshotpg * spp->oneshotpgs_per_blk;
		spp->pgs_per_oneshotpg = ONESHOT_PAGE_SIZE / (spp->pgsz);
			#define ONESHOT_PAGE_SIZE (FLASH_PAGE_SIZE * 3) 
				#define FLASH_PAGE_SIZE KB(16)
			spp->pgsz = spp->secsz * spp->secs_per_pg;
				spp->secsz = LBA_SIZE;
					#define LBA_SIZE (1 << LBA_BITS)
					#define LBA_BITS (9) // 512B
				spp->secs_per_pg = 4096 / LBA_SIZE;
		spp->oneshotpgs_per_blk = DIV_ROUND_UP(blk_size, ONESHOT_PAGE_SIZE); // 나누고 올림
			blk_size = DIV_ROUND_UP(capacity, spp->blks_per_pl * spp->pls_per_lun * spp->luns_per_ch * spp->nchs); // capacity 나누기 (blocks_per_nchs, 즉 총 블럭 수)
				capacity = 님이 할당한 SSD 용량(Ex: 1GB)
				#define BLKS_PER_PLN (1024)
				#define PLNS_PER_LUN (1)
				#define LUNS_PER_NAND_CH (4)
				#define NAND_CHANNELS (4)


write_buffer가 뭔지 알아야 할 듯
#define GLOBAL_WB_SIZE (NAND_CHANNELS * LUNS_PER_NAND_CH * ONESHOT_PAGE_SIZE * 2)
채널 NAND_CHANNELS개가 있고
채널마다 다이를 LUNS_PER_NAND_CH개 가지고
둘을 곱하면 SSD 내 다이의 수를 얘기하며
각 다이에 ONESHOT_PAGE_SIZE를 쓸건데
버퍼는 교대근무(하나 쓰고 하나 그동안 받고 교대하고..)하기 때문에 2를 곱함
근데 write_buffer가 실질적으로 일을 하는게 없이 걍 시간만 반환하고 앉아있음
ㄴ io.c에서 증명되는데, __do_perform_io 함수는 SQ(Submission Queue)로부터 io 요청을 하나 읽어서 처리하는데
    write 요청을 받았을 때 write buffer에 쓰지 않고 write해야 할 내용을 써야할 주소에 직접 memcpy(-) 하고 있음 -> memcpy가 write_buffer 영역에 이루어지는 것은 아닌지?
    while (remaining) 루프 확인..
-> 실제 복사 행위는 io.c에서 이루어지고, NVMeVirt에서의 버퍼는 실제로 -
write되는 "size" -> 언제 버퍼가 꽉 차서 쫙 쓰기가 일어나는지 타이밍을 잡기 위해서,
그리고 write될 때마다 각 page가 복사되는 등의 실제 버퍼를 쓴다 가정했을 때 소요되는 시간을 모방하여 계산,
해주기만 한다. (내피셜)

line의 크기 구분(TLC = SLC * 3)은 어떻게 이루어지냐?
1. advance_write_pointer 함수 내에서는 "wpp->curline->vpc == spp->pgs_per_line" 라는 조건이 있는데, 
이 spp->pgs_per_line 을 spp->pgs_per_line_slc(TLC의 1/3)으로 바꿔버리면 write pointer가 SLC 라인 크기만큼 쓰고나서 이동할 것임 
-> advance_write_pointer_slc 따로 구현을 하고
wp가 slc/tlc line 길이만큼 모으면 "어느 영역부터 slc_line 길이(혹은 tlc_line 길이)만큼 write 해줘라" 요청하면 될 듯

write_pointer := 4K씩 모으다가 line 단위로 다 모이면 모아둔거 통째로 write하고 free_line받아서 다음 write부터는 새로 받은 line에 해주는 녀석
-> write_pointer는 USER_IO 따로, GC_IO 따로 처리하도록 각각 존재함
=> MIG_IO 따로?! -> 원래는 필요할 줄 알았는데 이건 굳이임. GC_IO로 migration/GC 둘 모두 처리 가능(그냥 line manager만 두 개 두면 됨)

write_pointer는 2가지가 있고 하는 일은 다음과 같음

USER_IO 처리하는 write_pointer(slm으로부터 free line 받음)
-> 유저로부터 들어오는 page write 요청들을 SLC line 단위로 모아서 SLC cache에 write
=> 정확히 말하면 모아서 write가 아니라, write_pointer가 가리키는 곳에 page를 write하며, write 후 write_pointer가 바로 다음 부분을 가리키도록 할 뿐임.
즉 요청마다 특정 ssd 영역에 쭉 이어서 write하다가, write_pointer가 line의 끝에 도달하면,
쓰고 있던 line을 line_mgmt에 넘겨버리고(어느 라인 리스트에 들어갈지는 둘로 나뉨-> full: ipc == 0, vpc == page_per_line | victim: ipc > 0, vpc < page_per_line)
새로운 line을 line_mgmt의 free_list로부터 받은 후 write_pointer가 해당 라인의 시작을 가리키도록 함

GC_IO 처리하는 write_pointer(tlm으로부터 free line 받음)
1. SLC 영역에서 migration 대상으로 선택된 SLC_victim_line내의 valid_page들을 write함
2. TLC 영역에서 GC 대상으로 선택된 TLC_victim_line내의 valid_page들을 write함

write_pointer는 line_mgmt로부터 free_line을 받고, 그 free_line에 write함
즉 write_pointer가 write되는 영역은 ssd 내부 영역
또한, write_pointer 내부 페이지는 invalid 될 수 있음
이는 advance_write_pointer 구현 확인해보면 됨
아무튼, invalid된 페이지 존재하면 victim_line_list에 넣어버리고
아무런 invalid 페이지도 없으면 full_line_list에 넣어버림
여기서 문제가 생기는데
SLC cache에서 migration되는 line은 SLC line임
SLC line은 TLC line의 1/3 크기
TLC가 관리하는 victim_line_list(혹은 full_line_list)에 넣어버리려면 어떡하지?
선택된 SLC line을 page단위로 읽어보면서, valid page마다 TLC의 wpp에게 write요청을 넣어버릴까?

conv_write가 어떻게 동작하냐
두 가지로 나뉜다
1. 시간 계산
2. 실제 write<<<io.c쪽으로 알아서 schedule되게 짬때림

이게 괴리가 있는데, 가상의 버퍼가 있다고 생각하고 그 버퍼의 "남은 size"를 관리함.
write가 발생하면, 해당 버퍼의 남은 size를 보고, 한 번 write한 만큼 시간을 추가함
실제 write는 언제 일어나냐
일단 page를 wp의 line에 계속해서 쓰다가
wp가 관리하는 line이 꽉 차면
schedule_internal_operation으로 I/O 요청을 넣어둠
이 I/O 요청은 알아서 스케쥴링되면서 처리됨
그건 알겠는데 데이터가 I/O 요청이랑 묶여서 처리되어야 하는데 그게 안보임
io.c의 __do_perform_io에서, 특정 영역에 memcpy(flash memory로의 write를 모방)되는 것까지 보임
write 요청은 wp가 관리하는 line으로 일어나니까 wp가 관리하는 line에 write되고 있다는 소리겠지
그러면 wp에 실제로 모여있으려나?
<<<< 질문각 씨게 뜸

각 wp가 받는 것
USER_IO, GC_IO
USER_IO: 유저(CPU)의 write 요청, slm으로부터 free line을 받고 해당 라인에 쭉 써내림
GC_IO: GC 혹은 migration이 발동했을 때 선택된, victim_line 내부 각각의 valid page에 대한 write 요청, tlm으로부터 free line을 받고 해당 라인에 쭉 써내림

다음부터 할 거
ssd 파라미터들이 죄다 곱창나있으니 ssd_init_params 잔뜩 수정해야 함


ssd 파라미터 정의 바꾸기 위한 SLC vs TLC 구분

둘은 어디서부터 달라지는가?
-> oneshot page의 size가 달라진다
-> 또한 각 영역에 읽기/쓰기/지우기 등의 latency 차이도 존재한다
in ssdparams...
	int secsz; /* sector size in bytes */
	int secs_per_pg; /* # of sectors per page */
	int pgsz; /* mapping unit size in bytes*/
	int pgs_per_flashpg; /* # of pgs per flash page */
	int flashpgs_per_blk; /* # of flash pages per block */
	int pgs_per_oneshotpg; /* # of pgs per oneshot page */
	int oneshotpgs_per_blk; /* # of oneshot pages per block */
	int pgs_per_blk; /* # of pages per block */
	int blks_per_pl; /* # of blocks per plane */
	int pls_per_lun; /* # of planes per LUN (Die) */
	int luns_per_ch; /* # of LUNs per channel */
	int nchs; /* # of channels in the SSD */

여기서 나오는 ssd의 크고 작은 단위는 다음과 같고
sector : page를 나눈 단위 (보통 8등분, 512B)
page : 매핑 테이블의 단위 (4KB)
flashpage : 낸드 플래시의 쓰기 단위, 페이지를 가로로 모아서 만들어짐 (16KB)
oneshotpage : WL 하나에 들어가는 페이지 수, 페이지를 세로로 모아서 만들어짐 (SLC : 1 pgs, MLC : 2 pgs, TLC : 3, QLC : 4)
block : WL을 특정 단위로 모아둔 단위
plane : block의 집합
lun(=die) : plane의 집합
channel : lun의 집합
+ line(=superblock) : 모든 plane의 특정 WL에 속하는 page들의 집합
** line이 관리하는 page 수도 SLC/TLC 서로 다름(WL 기준이기 때문)

SLC와 TLC는 oneshot page size가 다르다. (SLC : 1, TLC : 3)
따라서 oneshot page를 모아둔 block이 달라진다.
plane 수, die 수, channel의 수가 달라지지는 않는다.

ssd 내의 total line 수는 달라진다.
전체 ssd 내 SLC Cache 비율이 늘어날 수록 total line 수가 줄어들기 때문..

line 당 WL 수는 고정이니까 line 당 block 수는 같다.
하지만 line 당 page 수는 달라진다.

(block보다 상위 단위 당) page 수를 따질 이유는 없다.
그렇게 셀 상황이 있나? 있으면 따져야지.
근데 그렇진 않은 것으로 일단 보임
따라서 신경써야 할 것은 block과 그 하위 단위들간의 관계이다.
그럼에도 상위 단위들을 신경 써야되려나 싶긴 한데 일단 스킵하고 필요하면 만들도록 하자
아무튼 이를 정리하면 SLC Cache를 위해 따로 추가해야 할 파라미터들은 다음과 같다

oneshotpgs_per_blk (block 당 oneshot page 수)
flashpgs_per_blk (block 당 flashpg 수) -> 이거 달라지냐?...->ㅇㅇ, 블럭 당 WL 개수는 고정인데 WL 당 flashpgs는 TLC/SLC 귀속
pgs_per_blk (block 당 page 수)
secs_per_blk (block 당 sector 수)
pgs_per_oneshotpg (oneshot page 당 page 수)

그리고 위 5가지를 기존에 계산에 사용하던 파라미터들도 수정이 필요하게 된다
간접적으로 영향받는 파라미터들은 다음과 같다...

oneshotpgs_per_blk: flashpgs_per_blk, pgs_per_blk
flashpgs_per_blk: X
pgs_per_blk: secs_per_blk, pgs_per_pl, pgs_per_line
secs_per_blk: secs_per_pl -> secs_per_lun -> secs_per_ch -> tt_secs
pgs_per_pl: pgs_per_line -> pgs_per_ch -> tt_pgs
pgs_per_line: secs_per_line
pgs_per_oneshotpg: pgs_per_blk

총 line 수는 같음.
다만 총 line을 SLC Cache/TLC 영역이 나눠먹음.
나눠먹은 line 수를 관리할 필요가 있어보임
지금은 init_lines_slc 함수 내에서 정해지는 걸로 되어있다만..

그리고 tt_* 들도 다 달라짐

또 뭐가 있을까.. 내일은 ssdparams 확실하게 조정하고, 각 함수마다 플래그를 추가해둬서 조정되게 하자? 근데 플래그를 추가하면 호환이 안되지 않나?
플래그를 conv_ftl 아니면 적당한 어딘가의 구조체에 숨겨놓을까???
함수 포인터?
매크로?
플래그?
플래그라면 어디에? 함수 인자로? 아니면 conv_ftl에?
이건 내일하자 근데 conv_ftl에 숨기는거 괜찮아 보이기는 하네
그렇게 한다면 플래그가 언제 바뀌는지를 판단하는 로직 필요해보이고..
slc_mode를 추가해서 현재 ssd에 SLC cache를 사용하는지 넣어두자
slc_mode는 정적으로 활용되겠지?
동적으로 현재 작업이 tlc영역으로의 작업인지, slc cache로의 작업인지 구분하는건 뭘로 할까?
현재 작업에 어느 write_pointer가 사용되어야 하는지를 기준으로 바뀌어야 하겠지?
conv_ftl에 이걸 넣어두면 migration할 때 꼬이지 않으려나?

함수 나눠둔거는 플래그로써 필요한 부분만 나누고
수정이 크게 필요한 함수는 걍 _slc 접미사 붙혀서 만들고, 플래그를 분기로 써먹고 호출하도록 하자
호환을 고려하려면 어떻게 해야할까
ssd_init_params는 _slc 붙혀서 새로 만들어야 겠는데?

/* wei edited - for SLC cache */
	int pg_4kb_rd_lat_slc;
	int pg_rd_lat_slc;
	int pg_wr_lat_slc;
	int blk_er_lat_slc;

	int flashpgs_per_blk_slc;
	int oneshotpgs_per_blk_slc;
	int pgs_per_oneshotpg_slc;
	int pgs_per_blk_slc;
	int secs_per_blk_slc;

	int secs_per_line_slc;
	int pgs_per_line_slc;

	int tt_lines_tlc;
	int tt_lines_slc;
/* wei edited end */

실제로 할당되어야 할 영역은 3G
내가 SLC Cache를 영역의 10%만큼 잡음으로써
2.7 + 0.1 = 2.8G만 사용하게 됨
따라서 실제 NVMeVirt 내 lpn은 2.8G만큼의 범위만 가짐
tt_pgs = 183552., 3,007,315,968B
2.8 * 1G(2^30B) = 3,006,477,107.2
뭐 실제로는 조금 더 되긴 하나본데

183,680의 start_lpn이 발생했고 이는 tt_pgs를 넘어선 값임
따라서 오류가 나는 것은 당연하긴 하네
그럼 어떡하지?

size를 쌩으로 나눠도 되는가?

(PORTION으로 나눠둔 라인) * pgs_per_line_each
==?
PORTION으로 기존 size를 줄이기
tt_lines_slc = tt_lines * 10 / 100
tt_lines_tlc = tt_lines - tt_lines_slc;


pgs_per_line * tt_lines_tlc + pgs_per_line_slc * tt_lines_slc
==
(TLC 라인 당 페이지 수) * (tt_lines - tt_lines_slc) + (TLC 라인 당 페이지 수 / 3) * (tt_lines_slc) // TLC 라인 당 페이지 수는 무조건 3배수
==
tt_lines * (TLC 라인 당 페이지 수) - (tt_lines * SLC_PORTION / 100) * (TLC 라인 당 페이지 수 / 3) * 2
!= ( 나중에 버그 날 듯, 해결 필요 일단 지금은 문제가 아닌데..)
3G * 0.1 * 1/3 + 3G * 0.9
== 3G - (3G * 10% * 2/3) 



SLC's tt_lines: 102
TLC's tt_lines: 922

USER_IO 보장.
SLC_MODE 보장.